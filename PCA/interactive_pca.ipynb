{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db34672b",
   "metadata": {},
   "source": [
    "# Interactive PCA \n",
    "\n",
    "In case there is an already existing PCA and dataset skip all the steps except the last one, that is the interactive part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6aeb7c",
   "metadata": {},
   "source": [
    "## Load all the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73395e9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Line2D\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "\n",
    "# Custom functions/classes\n",
    "path_to_module = './'  # Path where all the .py files are, relative to the notebook folder\n",
    "sys.path.append(path_to_module)\n",
    "\n",
    "from class_dataset import RandomCrop, Subtract, ToTensor, myDataset\n",
    "from load_data import DataProcesser\n",
    "from utils_app import frange, model_output_app\n",
    "import results_model\n",
    "from utils import model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874b9d0",
   "metadata": {},
   "source": [
    "## Set the files and variables\n",
    "\n",
    "It is best to leave most variables as they are. Important is to set the paths to the zip archive with the data and the .pytorch model file. \n",
    "\n",
    "A name needs to be given to the analysis, this is just to store all results of one pca in a separate file to prevent files being overwritten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myseed = 7\n",
    "torch.manual_seed(myseed)\n",
    "torch.cuda.manual_seed(myseed)\n",
    "np.random.seed(myseed)\n",
    "\n",
    "# Inputs\n",
    "data_file = './ERKH/ERKH.zip'\n",
    "model_file = './model/ERKKTR_model.pytorch'\n",
    "\n",
    "# Name of the output files\n",
    "name = 'ERKH'\n",
    "\n",
    "\n",
    "# Leave at None for automatic detection\n",
    "start_time = None\n",
    "end_time = None\n",
    "measurement = None\n",
    "selected_classes = None\n",
    "batch_sz = 2048  # set as high as GPU memory can handle for speed up, automatic detection not possible\n",
    "\n",
    "rand_crop = True\n",
    "set_to_project = 'all'  # one of ['all', 'train', 'validation', 'test']\n",
    "\n",
    "\n",
    "# Leave this, more than 2 principal components are not currently possible. I was too ambitious\n",
    "n_pca = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71831b0c",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e776f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = torch.load(model_file) if torch.cuda.is_available() else torch.load(model_file, map_location='cpu')\n",
    "net.eval()\n",
    "net.double()\n",
    "net = net.to(device)\n",
    "length = net.length\n",
    "print('LENGTH IS ', length)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Data Loading, Subsetting, Preprocessing\n",
    "perc_selected_ids = 1  # Select only percentile of all trajectories, not always useful to project them all and slow\n",
    "data = DataProcesser(data_file, datatable=False)\n",
    "measurement = data.detect_groups_times()['groups'] if measurement is None else measurement\n",
    "start_time = data.detect_groups_times()['times'][0] if start_time is None else start_time\n",
    "end_time = data.detect_groups_times()['times'][1] if end_time is None else end_time\n",
    "data.subset(sel_groups=measurement, start_time=start_time, end_time=end_time)\n",
    "\n",
    "data.get_stats()\n",
    "data.split_sets()\n",
    "classes = tuple(data.classes.iloc[:,1])\n",
    "dict_classes = data.classes[data.col_classname]\n",
    "\n",
    "# Check that the measurements columns are numeric, if not try to convert to float64\n",
    "cols_to_check = '^(?:{})'.format('|'.join(measurement))  # ?: for non-capturing group\n",
    "cols_to_check = data.dataset.columns.values[data.dataset.columns.str.contains(cols_to_check)]\n",
    "cols_to_change = [(s,t) for s,t in zip(cols_to_check, data.dataset.dtypes[cols_to_check]) if not pd.api.types.is_numeric_dtype(data.dataset[s])]\n",
    "if len(cols_to_change) > 0:\n",
    "    warnings.warn('Some measurements columns are not of numeric type. Attempting to convert the columns to float64 type. List of problematic columns: {}'.format(cols_to_change))\n",
    "    try:\n",
    "        cols_dict = {s[0]:'float64' for s in cols_to_change}\n",
    "        data.dataset = data.dataset.astype(cols_dict)\n",
    "    except ValueError:\n",
    "        warnings.warn('Conversion to float failed for at least one column.')\n",
    "\n",
    "data.get_stats()\n",
    "if selected_classes is not None:\n",
    "    data.dataset = data.dataset[data.dataset[data.col_class].isin(selected_classes)]\n",
    "# Suppress the warning that data were not processed, irrelevant for the app\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    data.split_sets(which='dataset')\n",
    "\n",
    "print('Start time: {}; End time: {}; Measurement: {}'.format(start_time, end_time, measurement))\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# df used to plot measurements\n",
    "assert set_to_project in ['all', 'train', 'validation', 'test']\n",
    "if set_to_project == 'all':\n",
    "    df = deepcopy(data.dataset)\n",
    "elif set_to_project == 'train':\n",
    "    df = deepcopy(data.train_set)\n",
    "elif set_to_project == 'validation':\n",
    "    df = deepcopy(data.validation_set)\n",
    "elif set_to_project == 'test':\n",
    "    df = deepcopy(data.test_set)\n",
    "print('Size of raw dataframe: {}'.format(df.shape))\n",
    "df.rename(columns={data.col_id: 'ID', data.col_class: 'Class'}, inplace = True)\n",
    "selected_ids = np.random.choice(df.loc[:,'ID'].unique(), round(perc_selected_ids * df.shape[0]), replace=False)\n",
    "df = df[df['ID'].isin(selected_ids)]\n",
    "print('Size of selected dataframe: {}'.format(df.shape))\n",
    "\n",
    "if batch_sz == -1:\n",
    "    batch_sz = round(df.shape[0]/10)\n",
    "print('Batch size: {}'.format(batch_sz))\n",
    "assert batch_sz <= df.shape[0]\n",
    "# Split for each measurement, melt and append.\n",
    "ldf = []\n",
    "for meas in measurement:\n",
    "    col_meas = [i for i in df.columns if re.match('^{}_'.format(meas), i)]\n",
    "    temp = df[['ID', 'Class'] + col_meas].melt(['ID', 'Class'])\n",
    "    temp['Time'] = temp['variable'].str.extract('([0-9]+$)')\n",
    "    temp['variable'] = temp['variable'].str.replace('_[0-9]+$', '', regex=True)\n",
    "    ldf.append(temp)\n",
    "df = pd.concat(ldf)\n",
    "del temp\n",
    "del ldf\n",
    "df.sort_values(['ID', 'Time'])\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Prepare dataloader for t-SNE, set high batch_size to speed up\n",
    "subtract_numbers = [data.stats['mu'][meas]['train'] for meas in measurement]\n",
    "if rand_crop:\n",
    "    ls_transforms = transforms.Compose([\n",
    "        Subtract(subtract_numbers),\n",
    "        RandomCrop(output_size=length, ignore_na_tails=True, export_crop_pos=True),\n",
    "        ToTensor()])\n",
    "else:\n",
    "    ls_transforms = transforms.Compose([\n",
    "        Subtract(subtract_numbers),\n",
    "        ToTensor()])\n",
    "\n",
    "mydataset = myDataset(dataset=data.dataset[data.dataset['ID'].isin(selected_ids)], transform=ls_transforms)\n",
    "mydataloader = DataLoader(dataset=mydataset,\n",
    "                          batch_size=batch_sz,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Classes object definition\n",
    "classes = tuple(data.classes[data.col_classname])\n",
    "classes_col = data.classes[data.col_classname]\n",
    "if selected_classes is not None:\n",
    "    classes = [i for i in classes if i\n",
    "               in list(data.classes[data.classes[data.col_class].isin(selected_classes)][data.col_classname])]\n",
    "    classes = tuple(classes)\n",
    "    classes_dict = data.classes[data.classes[data.col_class].isin(selected_classes)].to_dict()[data.col_classname]\n",
    "else:\n",
    "    classes_dict = data.classes.to_dict()[data.col_classname]\n",
    "\n",
    "net.batch_size = batch_sz  # Learn representations over the whole dataset at once if equal to dataset length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf32139",
   "metadata": {},
   "source": [
    "## PCA with all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net\n",
    "dataloader = mydataloader\n",
    "\n",
    "df_out = model_output_app(model, dataloader, export_prob=True, export_feat=True, device=device, export_crop_pos=rand_crop)\n",
    "df_out['Class'].replace(classes_col, inplace=True)\n",
    "feat_cols = [i for i in df_out.columns if i.startswith('Feat_')]\n",
    "feature_blobs_array = np.array(df_out[feat_cols])\n",
    "pca = PCA(n_components=n_pca)\n",
    "pca_original = pca.fit_transform(feature_blobs_array)\n",
    "\n",
    "out_dir = './results_' + name + '/pca'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "\n",
    "df_raw = pd.DataFrame(pca_original)\n",
    "df_original = df_raw.join(df_out)\n",
    "df_original.to_csv(out_dir + '/pca_all.csv', index=False)\n",
    "df_original = pd.DataFrame(df_original)\n",
    "\n",
    "Nclasses = len(np.unique(df_original['Class']))\n",
    "cmap = plt.cm.get_cmap('hsv')\n",
    "\n",
    "\n",
    "label_color_dict = {label: cmap(np.linspace(0.2,1,Nclasses))[idx] for idx, label in enumerate(np.unique(df_original['Class']))}\n",
    "#colors = [cmap(label_color_dict[label]) for label in df_original['Class']]\n",
    "\n",
    "colors = [label_color_dict[label] for label in df_original['Class']]\n",
    "#colors = df_original['Class'].astype(str).map(colors)\n",
    "\n",
    "\n",
    "plt.scatter(df_original[0], df_original[1], alpha=0.1, c=colors)\n",
    "custom_lines = [Line2D([0], [0], color=cmap(np.linspace(0.2,1,Nclasses))[i], lw=4) for i, cl in enumerate(cmap(np.linspace(0,1,Nclasses)))]\n",
    "#custom_lines = [Line2D([0], [0], color=label_color_dict[cl], lw=4) for cl in label_color_dict.keys()]\n",
    "plt.legend(custom_lines, label_color_dict.keys(), loc='upper right')\n",
    "\n",
    "# Add the axis labels\n",
    "plt.xlabel('PC 1 (%.2f%%)' % (pca.explained_variance_ratio_[0]*100))\n",
    "plt.ylabel('PC 2 (%.2f%%)' % (pca.explained_variance_ratio_[1]*100))\n",
    "plt.title('PCA-plot')\n",
    "plt.savefig(out_dir + '/pca_all.pdf', format='pdf')\n",
    "# Close the plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa6817",
   "metadata": {},
   "source": [
    "## Set variables for the partial PCAs\n",
    "\n",
    "Change the percentile to something mangable for the interactive PCA. Too many datapoints and it might crash. \n",
    "\n",
    "The threshold_confidence is the lowest value that the uncorrelated trajectories can take. This is to identify what a realistic trajectory in each class should look like. Setting it around 75% would be good, that way the picked trajectories not too clean but still representative. But of course the threshold might depend on if you think your model is over- or underfitted. An overfitted model might benefit from a lower confidence and an underfitted one from a higher one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal confidence for uncorrelated prototypes\n",
    "perc_selected_ids = 0.01  # Select only percentile of all trajectories, not always useful to project them all and slow\n",
    "threshold_confidence = 0.75\n",
    "\n",
    "\n",
    "# Leave the rest as is\n",
    "# Helper functions for plotting\n",
    "col_id = data.col_id\n",
    "col_class = data.col_class\n",
    "col_classname = data.col_classname\n",
    "\n",
    "\n",
    "# Convert percentile to actuall number of datapoints\n",
    "length_data = len(pd.DataFrame(df_original))\n",
    "npoint = round(perc_selected_ids * length_data)\n",
    "ntop = npoint\n",
    "nworst = npoint\n",
    "nuncor = npoint\n",
    "nrandom = npoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3183833",
   "metadata": {},
   "source": [
    "## PCA with the top class trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Top trajectories per class\n",
    "tops = results_model.top_confidence_perclass(model, dataloader, n=ntop, labels_classes=dict_classes)\n",
    "\n",
    "df_tops = df_original.loc[df_original['ID'].isin(tops['ID'])]\n",
    "df_tops.to_csv(out_dir + '/pca_tops_' + str(config_pca['perc_selected_ids']) + '.csv', index=False)\n",
    "\n",
    "colors = [label_color_dict[label] for label in df_tops['Class']]\n",
    "plt.scatter(df_tops[0], df_tops[1], alpha=0.1, c=colors)\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color=cmap(np.linspace(0.2,1,Nclasses))[i], lw=4) for i, cl in enumerate(cmap(np.linspace(0.2,1,Nclasses)))]\n",
    "plt.legend(custom_lines, label_color_dict.keys(), loc='upper right')\n",
    "\n",
    "# Add the axis labels\n",
    "plt.xlabel('PC 1 (%.2f%%)' % (pca.explained_variance_ratio_[0]*100))\n",
    "plt.ylabel('PC 2 (%.2f%%)' % (pca.explained_variance_ratio_[1]*100))\n",
    "plt.title('PCA-plot tops')\n",
    "plt.savefig(out_dir + '/pca_tops_' + str(config_pca['perc_selected_ids']) + '.pdf', format='pdf')\n",
    "# Close the plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4cb527",
   "metadata": {},
   "source": [
    "## PCA with the least correlated class trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Least correlated set per class\n",
    "uncorr = results_model.least_correlated_set(model, dataloader, n=nuncor, labels_classes=dict_classes, threshold_confidence=threshold_confidence)\n",
    "\n",
    "df_uncorr = df_original.loc[df_original['ID'].isin(uncorr['ID'])]\n",
    "df_uncorr.to_csv(out_dir + '/pca_uncorr_' + str(config_pca['perc_selected_ids']) + '.csv', index=False)\n",
    "\n",
    "colors = [label_color_dict[label] for label in df_uncorr['Class']]\n",
    "plt.scatter(df_uncorr[0], df_uncorr[1], alpha=0.1, c=colors)\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color=cmap(np.linspace(0.2,1,Nclasses))[i], lw=4) for i, cl in enumerate(cmap(np.linspace(0.2,1,Nclasses)))]\n",
    "plt.legend(custom_lines, label_color_dict.keys(), loc='upper right')\n",
    "\n",
    "# Add the axis labels\n",
    "plt.xlabel('PC 1 (%.2f%%)' % (pca.explained_variance_ratio_[0]*100))\n",
    "plt.ylabel('PC 2 (%.2f%%)' % (pca.explained_variance_ratio_[1]*100))\n",
    "plt.title('PCA-plot uncorr')\n",
    "plt.savefig(out_dir + '/pca_uncorr_' + str(config_pca['perc_selected_ids']) + '.pdf', format='pdf')\n",
    "# Close the plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7e68f",
   "metadata": {},
   "source": [
    "## PCA with the worst class trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b361a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Worst trajectory per class\n",
    "worsts = results_model.worst_classification_perclass(model, dataloader, n=nworst, labels_classes=dict_classes)\n",
    "\n",
    "df_worsts = df_original.loc[df_original['ID'].isin(worsts['ID'])]\n",
    "df_worsts.to_csv(out_dir + '/pca_worsts_' + str(perc_selected_ids) + '.csv', index=False)\n",
    "\n",
    "colors = [label_color_dict[label] for label in df_worsts['Class']]\n",
    "plt.scatter(df_worsts[0], df_worsts[1], alpha=0.1, c=colors)\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color=cmap(np.linspace(0.2,1,Nclasses))[i], lw=4) for i, cl in enumerate(cmap(np.linspace(0.2,1,Nclasses)))]\n",
    "plt.legend(custom_lines, label_color_dict.keys(), loc='upper right')\n",
    "\n",
    "# Add the axis labels\n",
    "plt.xlabel('PC 1 (%.2f%%)' % (pca.explained_variance_ratio_[0]*100))\n",
    "plt.ylabel('PC 2 (%.2f%%)' % (pca.explained_variance_ratio_[1]*100))\n",
    "plt.title('PCA-plot worst')\n",
    "plt.savefig(out_dir + '/pca_worsts_' + str(perc_selected_ids) + '.pdf', format='pdf')\n",
    "# Close the plot\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbf98a",
   "metadata": {},
   "source": [
    "## PCA with random class trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ba473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Random sample\n",
    "# Get a random sample of trajectories.\n",
    "randoms_ids = []\n",
    "for classe in data.validation_set['class'].unique():\n",
    "    randoms_ids += list(data.dataset.loc[data.dataset['class']==classe]['ID'].sample(nrandom))\n",
    "randoms = model_output(model, dataloader)\n",
    "randoms = randoms.loc[randoms['ID'].isin(randoms_ids)]\n",
    "\n",
    "df_randoms = df_original.loc[df_original['ID'].isin(randoms['ID'])]\n",
    "df_randoms.to_csv(out_dir + '/pca_randoms_' + str(perc_selected_ids) + '.csv', index=False)\n",
    "\n",
    "colors = [label_color_dict[label] for label in df_randoms['Class']]\n",
    "plt.scatter(df_randoms[0], df_randoms[1], alpha=0.1, c=colors)\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color=cmap(np.linspace(0.2,1,Nclasses))[i], lw=4) for i, cl in enumerate(cmap(np.linspace(0.2,1,Nclasses)))]\n",
    "plt.legend(custom_lines, label_color_dict.keys(), loc='upper right')\n",
    "# Add the axis labels\n",
    "plt.xlabel('PC 1 (%.2f%%)' % (pca.explained_variance_ratio_[0]*100))\n",
    "plt.ylabel('PC 2 (%.2f%%)' % (pca.explained_variance_ratio_[1]*100))\n",
    "plt.title('PCA-plot randoms')\n",
    "plt.savefig(out_dir + '/pca_randoms_' + str(perc_selected_ids) + '.pdf', format='pdf')\n",
    "# Close the plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd6748",
   "metadata": {},
   "source": [
    "## Interactive PCA with existing PCA results from above\n",
    "\n",
    "This is where the interactive PCA will be displayed. There is two options to do this, with the data from above or with already existing results from previous calculations. \n",
    "\n",
    "If your doing it with the data from above, what would need to be changed which of the PCA you would want to use. Generally speaking, it is not recommended to use the full PCA since there will be too many datapoint and the interactive PCA might crash, so go any of the other 4 instead. \n",
    "\n",
    "Alternatively, change the input for the PCA results and the path to where the dataset is stored. The time_series is the dataset.csv file that was used for the previous parts. \n",
    "\n",
    "It is also possible to change the color scheme to any of the matplotlib schemes. Just change the name in the cmap variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b15e285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753b831269c8498786d3dc8e20065d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FigureWidget({\n",
       "    'data': [{'marker': {'color': [green, green, green, ..., blue, blue, blue],\n",
       "…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Output, VBox\n",
    "from IPython.display import display, HTML\n",
    "import http.server\n",
    "import socketserver\n",
    "import threading\n",
    "import webbrowser\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "### If you have already existing pca results and the dataset set their paths here manually\n",
    "pca = pd.read_csv('../pca_original.csv', sep=',', index_col=False)\n",
    "time_series = pd.read_csv('../ALLH_50intp.csv', sep=',', index_col=False)\n",
    "\n",
    "\n",
    "\n",
    "### If you already did pcas from above just use this and chose the pca results that you would like to use\n",
    "\n",
    "#time_series = data.datset\n",
    "\n",
    "#pca = df_original   #Full pca\n",
    "#pca = df_tops       #Only top results\n",
    "#pca = df_uncorr     #Only uncorrelated results\n",
    "#pca = df_worsts     #Only worst results\n",
    "#pca = df_randoms    #Only random results\n",
    "\n",
    "\n",
    "### Setting the colors scheme (any colorpalette from matplotlib is fine)\n",
    "cmap = plt.cm.get_cmap('hsv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------#\n",
    "# From here on leave as is\n",
    "\n",
    "Nclasses = len(np.unique(pca['Class']))\n",
    "#color_dict = {label: cmap(np.linspace(0.2,1,Nclasses))[idx] for idx, label in enumerate(np.unique(pca['Class']))}\n",
    "color_dict = {'WT': 'green', 'PIK3CA_H1047R': 'blue', 'ErbB2': 'lightgreen', 'Akt1_E17K': 'yellow', 'PTEN_del': 'purple', 'PIK3CA_E545K': 'lightblue'}\n",
    "\n",
    "\n",
    "components = pca[0:2]\n",
    "\n",
    "pca['colors'] = pca['Class'].apply(lambda x: color_dict.get(x))\n",
    "\n",
    "\n",
    "# Make the graphs\n",
    "fig = go.FigureWidget()\n",
    "fig.add_scatter(x=pca[\"0\"], \n",
    "    y=pca[\"1\"], \n",
    "    marker_color=list(pca.colors),\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, opacity=0.2),\n",
    "    text=list(pca.ID + ', ' + pca['Class'])\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PCA\",\n",
    "    xaxis_title=\"PC1\",\n",
    "    yaxis_title=\"PC2\",\n",
    "    width=800, height=800\n",
    "    )\n",
    "\n",
    "\n",
    "fig1 = fig.data[0]\n",
    "\n",
    "\n",
    "prob = pd.concat([pca['ID'], pca['Class'], pca.filter(regex='Prob_', axis=1)], axis=1)\n",
    "prob = round(prob, 2)\n",
    "\n",
    "colnames = list(time_series.columns.values)\n",
    "colnames.remove('ID')\n",
    "colnames.remove('class')\n",
    "groups = list(OrderedDict.fromkeys([i.split('_')[0] for i in colnames]))\n",
    "\n",
    "\n",
    "measurements = []\n",
    "\n",
    "for i in groups:\n",
    "    measurements.append(time_series.filter(regex=i))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "out = Output()\n",
    "@out.capture(clear_output=True)\n",
    "def display_timeseries(trace, points, state):\n",
    "    \n",
    "    ts = make_subplots(rows=2, cols=2,  \n",
    "        subplot_titles=['PCA', 'ID: ' + time_series.loc[points.point_inds[0],'ID'] + ' -> Class: '+ str(time_series.loc[points.point_inds[0],'class'])],\n",
    "        specs=[[{\"type\": \"scatter\",'rowspan': 2},\n",
    "            {\"type\": \"scatter\"}],\n",
    "            [None,\n",
    "            {\"type\": \"table\"}]],\n",
    "        row_width=[0.1, 0.5]\n",
    "        )\n",
    "    for i in range(len(measurements)):\n",
    "        #ts.add_trace(go.Scatter(x=list(range(0,(time_series.shape[1]-2)*5,5)),\n",
    "        #    y=time_series.iloc[points.point_inds[0],2:time_series.shape[1]],\n",
    "        #    marker_color=pca.colors[points.point_inds[0]]\n",
    "        #    ),row=1,col=2) marker_color=float('0.'+str(i))+0.1\n",
    "        ts.add_trace(go.Scatter(x=list(range(0,(measurements[i].shape[1])*5,5)),\n",
    "            y=measurements[i].iloc[points.point_inds[0],0:measurements[i].shape[1]],\n",
    "            name=groups[i]\n",
    "            ),row=1,col=2)\n",
    "\n",
    "    ts.add_trace(go.Scatter(x=pca[\"0\"], \n",
    "        y=pca[\"1\"], \n",
    "        marker_color=list(pca.colors),\n",
    "        mode='markers',\n",
    "        marker=dict(size=10, opacity=0.5),\n",
    "        text=list(pca.ID+', '+pca['Class'])\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    table=go.Figure(data=[go.Table(header=dict(values=list(prob.columns)),\n",
    "                cells=dict(values=[prob[col][points.point_inds[0]] for col in prob.columns]))])\n",
    "    ts.add_trace(table.data[0], row=2, col=2)\n",
    "    \n",
    "    \n",
    "    ts.add_trace(go.Scatter(x=points.xs, \n",
    "        y=points.ys, \n",
    "        marker_color='red',\n",
    "        mode='markers',\n",
    "        marker=dict(size=10, opacity=1)\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    \n",
    "    ts['layout']['xaxis'].update(title_text='PC1')\n",
    "    ts['layout']['yaxis'].update(title_text='PC2')\n",
    "    \n",
    "    ts['layout']['xaxis2'].update(title_text='Time in minutes')\n",
    "    ts['layout']['yaxis2'].update(title_text='Ratio')\n",
    "    \n",
    "    ts.update_layout(title='Table and Scatter Plot from Pandas DataFrame')\n",
    "\n",
    "    tsf = go.FigureWidget(ts)\n",
    "    \n",
    "    tsf.write_html(\"./ts.html\")\n",
    "    plot_file ='ts.html'\n",
    "    \n",
    "    def serve_plot():\n",
    "        handler = http.server.SimpleHTTPRequestHandler\n",
    "        with socketserver.TCPServer((\"\", 0), handler) as httpd:\n",
    "            port = httpd.server_address[1]\n",
    "            webbrowser.open(f\"http://localhost:{port}/{plot_file}\")\n",
    "            httpd.serve_forever()\n",
    "\n",
    "    server_thread = threading.Thread(target=serve_plot)\n",
    "    server_thread.daemon = True\n",
    "    server_thread.start()\n",
    "\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "\n",
    "    return f\"http://localhost:{port}/{plot_file}\"\n",
    "\n",
    "\n",
    "\n",
    "fig1.on_click(display_timeseries)\n",
    "\n",
    "VBox([fig, out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae3779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
