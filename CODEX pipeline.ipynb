{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6033925e",
   "metadata": {},
   "source": [
    "## Path variables and names\n",
    "- WD: the working directory where the input data is and the results will be stored\n",
    "- name: a specific name for the results, such that they can be distinguished from other runs\n",
    "- data: what the zipped file with the input data will be named or if it already exists what it is called\n",
    "\n",
    "## Training parameters\n",
    "Hyperparameters to set the model dimensions:\n",
    "- length: int or None. Length of the input time-series. If the input is multivariate, each channel will have the specified length. Setting it to a smaller value than the actual length of the trajectories can be used for data augmentation (see RandomCrop). If None, automatically detects the longest common length across al trajectories.\n",
    "- nclass: int or None. Number of output classes. If None, automatically detects the number of unique values in the class column of the dataset.\n",
    "- nfeatures: int, size of the input representation before the output layer. This also corresponds to the number of filters in the last convolution layer. Usually between 5 and 15.\n",
    "\n",
    "Hyperparameters for training:\n",
    "- nepochs: int, number of training epochs. Increase this if the loss curves are still decreasing at the end of the training. The range is very variable, but can be over hundreds.\n",
    "- batch: int, number of samples per batch. Conventionally, this is set to a power of 2.\n",
    "- L2_reg: float, L2 regularization factor. This helps to prevent overfitting by penalizing large weights in the model parameters. In general try to always have a mild regularization, say 1e-3. Increase if you face overfitting issues, decrease if you face underfitting.\n",
    "- lr: float, initial learning rate. This is the most important parameter to tweak to have a smooth learning curve. 1e-2 is usually a good starting value.\n",
    "\n",
    "By default, the learning rate is scheduled to decrease by a factor gamma at fixed epochs.\n",
    "- lr_decrease_schedule: list of integers or None, epochs at which to decrease the learning_rate. If None, the number of epochs will be evenly divided in bins of the same length. \n",
    "- lr_decrease_factor: float, factor by which the lr is multiplied at the specified epochs. Usually between 0.1 (divide by 10) and 0.5 (divide by 2).\n",
    "\n",
    "Other parameters:\n",
    "- ngpu: int, number of GPU used to perform the training. -1 means use all; 0 means use CPU only.\n",
    "- ncpu_LoadData: int, number of CPU cores used to load and preprocess the data that is passed to the network. This parameter is unrelated to `ngpu`: the model can be trained on GPU, while the data are loaded with CPU. If set too low, this parameter can become a bottleneck and slow down the training process.\n",
    "\n",
    "## Prototypes and PCA parameters\n",
    "- batch: how many trajectories can be loaded at once, set as high as memory allows\n",
    "- n_prototypes: number of prototypes for each group\n",
    "- threshold_confidence: the lowest class probability that the least correlated trajectories can take\n",
    "- perc_selected_ids: the percentage of point that should go into the PCAs of the subsets\n",
    "\n",
    "\n",
    "## Motifs extraction parameters\n",
    "- n_series_perclass: int, maximum number of series, per class, on which motif extraction is attempted.\n",
    "- n_pattern_perseries: int, maximum number of motifs to extract out of a single trajectory.\n",
    "- mode_series_selection: str one of ['top_confidence', 'least_correlated']. Mode to select the trajectories from which to extract the motifs (see Prototype analysis). If top confidence, the motifs might be heavily biased towards a representative subpopulation of the class. Hence, the output might not reflect the whole diversity of motifs induced by the class.\n",
    "- extend_patt: int, by how many points to extend motifs? After binarization into 'relevant' and 'non-relevant time points', the motifs are usually fragmented because a few points in their middle are improperly classified as 'non-relevant'. This parameter allows to extend each fragment by a number of time points (in both time directions) before extracting the actual patterns.\n",
    "- min_len_patt/max_len_patt: int, set minimum/maximum size of a motif. **/!\\ The size is given in number of time-points. This means that if the input has more than one channel, the actual length of the motifs will be divided across them.** For example, a motif that spans over 2 channels for 10 time points will be considered of length 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f62cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "\n",
    "config_path = './source'\n",
    "\n",
    "# open the config\n",
    "with open('./source/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "    \n",
    "# Set working directory/where data is\n",
    "WD = './AKTH'\n",
    "    \n",
    "# Update values as needed\n",
    "config['data'] = 'new_value_for_data'\n",
    "config['name'] = 'new_value_for_name'\n",
    "config['scripts'] = './source'\n",
    "\n",
    "# Update values in the 'prep' section\n",
    "config['prep']['training'] = 0.6\n",
    "config['prep']['validation'] = 0.25\n",
    "config['prep']['test'] = 0.15\n",
    "config['prep']['seed'] = 2\n",
    "\n",
    "# Update values in the 'training' section\n",
    "config['training']['nclass'] = ''\n",
    "config['training']['length'] = ''\n",
    "config['training']['nfeatures'] = 10\n",
    "config['training']['batch'] = 50\n",
    "config['training']['lr'] = 0.01\n",
    "config['training']['schedule'] = ''\n",
    "config['training']['gamma'] = 0.01\n",
    "config['training']['penalty'] = 0.001\n",
    "config['training']['measurement'] = ''\n",
    "config['training']['startTime'] = ''\n",
    "config['training']['endTime'] = ''\n",
    "config['training']['nepochs'] = 30\n",
    "config['training']['ngpu'] = 0\n",
    "config['training']['ncpuLoad'] = 8\n",
    "config['training']['seed'] = 7\n",
    "\n",
    "# Update values in the 'prototypes' section\n",
    "config['prototypes']['seed'] = 7\n",
    "config['prototypes']['batch'] = 2048\n",
    "config['prototypes']['n_prototypes'] = 5\n",
    "config['prototypes']['threshold_confidence'] = 0.75\n",
    "\n",
    "# Update values in the 'pca' section\n",
    "config['pca']['seed'] = 7\n",
    "config['pca']['batch'] = 2048\n",
    "config['pca']['perc_selected_ids'] = 0.1\n",
    "config['pca']['threshold_confidence'] = 0.75\n",
    "\n",
    "# Update values in the 'motif analysis' section\n",
    "config['motif analysis']['seed'] = 7\n",
    "config['motif analysis']['n_series_perclass'] = 10\n",
    "config['motif analysis']['n_pattern_perseries'] = 10\n",
    "config['motif analysis']['mode_series_selection'] = 'least_correlated'\n",
    "config['motif analysis']['thresh_confidence'] = 0.75\n",
    "config['motif analysis']['extend_patt'] = 0\n",
    "config['motif analysis']['min_len_patt'] = 0\n",
    "config['motif analysis']['max_len_patt'] = 200\n",
    "\n",
    "    \n",
    "\n",
    "# save the new config\n",
    "with open(WD + '/config.yaml', 'w') as file:\n",
    "        yaml.dump(config, file, default_flow_style=False)\n",
    "\n",
    "print('Configurations sucessfully updated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('snakemake -s \\'./source/Snakefile.txt\\' -d ' + WD + ' --configfile ' + WD + '/config.yaml' ' -c10 --latency-wait 120')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
